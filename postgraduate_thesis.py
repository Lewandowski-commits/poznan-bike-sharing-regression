# -*- coding: utf-8 -*-
"""postgraduate thesis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yg-X_PFXzxc817AoQkAZFP5dq5o044bw

# Library import
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import os
from google.colab import drive
import numpy as np
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, scale
from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score

sns.set_theme(style='whitegrid')
# set the figure size
plt.rcParams['figure.figsize'] = (10,10)

# mount google drive
drive.mount("/content/gdrive", force_remount=True)

"""# Data import

## Bike sharing data from ZTM
"""

path = "/content/gdrive/MyDrive/Praca podyplomowa - Michał Lewandowski - Big Data CDV 2020 2021/data/ZTM/"

os.chdir(path)

# show directory contents
files = [x for x in os.listdir(path) if '.XLSX' in x]
print(files)

def transform_data_file_bike(file_path):
  #extract the bike type and year for the data
  file_year = int(file_path.split(' ')[3])
  bike_type = file_path.split('-')[1].split('.')[0].strip()

  #create emtpy df variables
  rents = pd.DataFrame()
  returns = pd.DataFrame()

  #load in the files depending on the bike type and add the type as a column

  if bike_type == 'PRM3G':
    rents, returns = pd.read_excel(file_path,sheet_name='Wypożyczenia'), pd.read_excel(file_path,sheet_name='Zwroty')
    rents['drive_type'], returns['drive_type'] = 'conventional', 'conventional'
    rents['action'], returns['action'] = 'rent', 'return'
    rents['bike_type'], returns['bike_type'] = bike_type, bike_type
    rents['child_seat'], returns['child_seat'] = False, False
    rents['bike_size'], returns['bike_size'] = 'adult', 'adult'
  elif bike_type == 'PRM4G':
    rents_norm, returns_norm = pd.read_excel(file_path,sheet_name='Wypożyczenia - Zwykłe'), pd.read_excel(file_path,sheet_name='Zwroty - Zwykłe')
    rents_el, returns_el = pd.read_excel(file_path,sheet_name='Wypożyczenia - Zwykłe'), pd.read_excel(file_path,sheet_name='Zwroty - Zwykłe')
    rents_norm['drive_type'], returns_norm['drive_type'] = 'conventional', 'conventional'
    rents_el['drive_type'], returns_el['drive_type'] = 'electric', 'electric'

    rents = rents_norm.append(rents_el)
    returns = returns_norm.append(returns_el)

    rents['child_seat'], returns['child_seat'] = False, False
    rents['action'], returns['action'] = 'rent', 'return'
    rents['bike_type'], returns['bike_type'] = bike_type, bike_type
    rents['bike_size'], returns['bike_size'] = 'adult', 'adult'
  else:
    rents_to_split, returns_to_split = pd.read_excel(file_path,sheet_name='Wypożyczenia', header=1), pd.read_excel(file_path,sheet_name='Zwroty', header=1)
    rents_kids, returns_kids = rents_to_split.loc[:3], returns_to_split.loc[:3]
    rents_adults, returns_adults = rents_to_split.loc[6:], returns_to_split.loc[6:]

    rents_kids['child_seat'], rents_adults['child_seat'] = False, True
    returns_kids['child_seat'], returns_adults['child_seat'] = False, True
    rents_kids['bike_size'], returns_kids['bike_size'] = 'child', 'child'
    rents_adults['bike_size'], returns_adults['bike_size'] = 'adult', 'adult'

    rents = rents_kids.append(rents_adults)
    returns = returns_kids.append(returns_kids)

    rents['bike_type'], returns['bike_type'] = 'PRM3G', 'PRM3G'
    rents['drive_type'], returns['drive_type'] = 'conventional', 'conventional'
    rents['action'], returns['action'] = 'rent', 'return'

    #give the first column a name as it doesn't have one in the file
    rents.rename(columns={'Unnamed: 0':'location'}, inplace=True)
    returns.rename(columns={'Unnamed: 0':'location'}, inplace=True)

  result = rents.append(returns)
  result.rename(columns={'Lokalizacja':'location'}, inplace=True)

  result = result.drop('Sumarycznie', axis = 1)

  result = pd.melt(result,
                   id_vars=[x for x in result.columns if x in ['location', 'action', 'bike_type', 'drive_type', 'child_seat', 'bike_size']],
                   var_name = 'month',
                   value_name = 'action_count')
  
  month_mapping = {'Styczeń': 1,
                   'Luty': 2,
                   'Marzec': 3,
                   'Kwiecień': 4,
                   'Maj': 5,
                   'Czerwiec': 6,
                   'Lipiec': 7,
                   'Sierpień': 8,
                   'Wrzesień': 9,
                   'Październik': 10,
                   'Listopad': 11,
                   'Grudzień': 12}

  # transform the dates polish to a readable format
  result['month'] = result['month'].map(month_mapping)
  result['date'] = result['month'].apply(lambda x: datetime.datetime(file_year, int(x), 1))
  result = result.drop('month', axis=1)

  # remove NaNs 
  result = result.dropna()

  return result

df = pd.DataFrame()

for x in files:
  df = df.append(transform_data_file_bike(x))

"""## Weather data

2019 data taken from *Instytut Meteorologii i Gospodarki Wodnej* - [Rocznik Meteorologiczny](https://danepubliczne.imgw.pl/data/dane_pomiarowo_obserwacyjne/Roczniki/Rocznik%20meteorologiczny/Rocznik%20Meteorologiczny%202019.pdf), table 27, p. 79-80


---

2020 data taken from taken from *Instytut Meteorologii i Gospodarki Wodnej* - [Rocznik Meteorologiczny](https://danepubliczne.imgw.pl/data/dane_pomiarowo_obserwacyjne/Roczniki/Rocznik%20meteorologiczny/Rocznik%20Meteorologiczny%202020.pdf), table 26, p. 78-79

### Symbole użyte w tabelach:

* Pśr – średnia miesięczna wartość ciśnienia atmosferycznego
* Pmax – maksymalna wartość ciśnienia atmosferycznego
* Pmin – minimalna wartość ciśnienia atmosferycznego
* Tśr – średnia miesięczna wartość temperatury powietrza
* Tmaxśr – średnia maksymalna wartość temperatury powietrza
* Tminśr – średnia minimalna wartość temperatury powietrza
* ABS Tmax – absolutna maksymalna wartość temperatury powietrza
* ABS Tmin – absolutna minimalna wartość temperatury powietrza
* Uśr – średnia miesięczna wartość wilgotności względnej
* Umin – minimalna wartość wilgotności względnej
* dd – kierunek wiatru
* ffśr – średnia miesięczna wartość prędkości wiatru
* ffmax – maksymalna wartość prędkości wiatru
* Nśr – średnia miesięczna wartość zachmurzenia ogólnego
* Rdsuma – suma miesięczna opadów atmosferycznych
* Rdmax – maksymalna suma dobowa opadów atmosferycznych
* Tgminśr – średnia miesięczna wartość temperatury minimalnej przy powierzchni gruntu
* ABS Tgmin – absolutna temperatura minimalna przy powierzchni gruntu
* Ssuma – suma miesięczna usłonecznienia
"""

path = "/content/gdrive/MyDrive/Praca podyplomowa - Michał Lewandowski - Big Data CDV 2020 2021/data/IMGW/"

os.chdir(path)

# show directory contents
files = [x for x in os.listdir(path) if '.xlsx' in x]
print(files)

def transform_data_file_weather(file_path):
  #extract the bike type and year for the data
  file_year = int(file_path.split('-')[0].split(' ')[2])

  #load in first two sheets, drop empty columns, rename the unnamed one
  df_weather = pd.read_excel(file_path, header=0, sheet_name=0).append(pd.read_excel(file_path, header=0, sheet_name=1))
  df_weather = df_weather.dropna(axis = 1, how = 'all')
  df_weather = df_weather.dropna(axis = 0, how = 'all')
  df_weather.rename(columns={'Unnamed: 0': 'month'}, inplace=True)

  #remove the rows which indicate the day where max/min values happened
  df_weather = df_weather[(df_weather['month'] != 'DATA') & (df_weather['month'] != 'Liczba dni:') & (df_weather['month'] != 'Liczba dni ze zjawiskami meteorologicznymi:') & (df_weather['month'] != 'Ssuma')]

  #transpose the table
  df_weather = df_weather.transpose()
  df_weather = df_weather.reset_index()
  df_weather.columns =  df_weather.iloc[0]
  df_weather = df_weather[1:]
  df_weather = df_weather[df_weather['month'] != 'I-XII']

  #map the month names

  month_mapping = {'I': 1,
                   'II': 2,
                   'III': 3,
                   'IV': 4,
                   'V': 5,
                   'VI': 6,
                   'VII': 7,
                   'VIII': 8,
                   'IX': 9,
                   'X': 10,
                   'XI': 11,
                   'XII': 12}
  
  # transform the dates polish to a readable format
  df_weather['month'] = df_weather['month'].map(month_mapping)
  df_weather['date'] = df_weather['month'].apply(lambda x: datetime.datetime(file_year, int(x), 1))
  df_weather = df_weather.drop('month', axis=1)

  return df_weather

weather = transform_data_file_weather(files[0]).append(transform_data_file_weather(files[1]))

df = df.merge(weather, on='date', how='left')

df

#uncomment below line to save to csv
#df.to_csv('merged data.csv')

"""## Public holidays, trade Sundays

Public holidays data:
 - 2019 - [timeanddate.com](https://www.timeanddate.com/holidays/poland/2019?hol=9)
 - 2020 - [timeanddate.com](https://www.timeanddate.com/holidays/poland/2020?hol=9)
"""

def get_yearly_holidays(year: int):
  path = f'https://www.timeanddate.com/holidays/poland/{year}?hol=9'

  # sent a request to the site and convert to pandas df, drop NaNs and clean columns names
  holidays_df = pd.read_html(path,
                              header=1)[0]
  holidays_df = holidays_df.dropna()
  holidays_df = holidays_df.rename(columns={'Unnamed: 1': 'Weekday'})
  # add in the year and format the date column
  holidays_df['Date'] = f'{year} ' + holidays_df['Date'].apply(lambda x: x.split(' ')[0]) + ' 01'
  holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])

  # reformat the table, drop holiday name
  holidays_df_gpd = holidays_df.groupby(by=['Date', 'Type']).count().reset_index()
  holidays_df_gpd = holidays_df_gpd.drop('Name', axis = 1)
  holidays_df_gpd = holidays_df_gpd.pivot_table(index = ['Date'],
                                                columns = ['Type'])

  # flatten the column levels
  holidays_df_gpd.columns = holidays_df_gpd.columns.get_level_values(1)
  holidays_df_gpd = holidays_df_gpd.reset_index()

  # fill NaNs with 0, correct datatypes, rename cols
  holidays_df_gpd = holidays_df_gpd.fillna(value = 0)

  int_cols = ['National holiday', 'Observance', 'Season']
  holidays_df_gpd[int_cols] = holidays_df_gpd[int_cols].astype('int')

  holidays_df_gpd = holidays_df_gpd.rename(columns = dict(zip(int_cols, ['bank_holidays_cnt', 'observances_cnt', 'equinoxes_cnt'])))
  holidays_df_gpd = holidays_df_gpd.rename(columns = {'Date': 'date'})


  return holidays_df_gpd

holidays = get_yearly_holidays(2019).append(get_yearly_holidays(2019))

holidays.head()

df = df.merge(holidays,
              on='date',
              how='left')

df.head()

""" Trade Sundays data:
- 2019 - [kalendarzswiat.pl](https://www.kalendarzswiat.pl/niedziele_handlowe/2019)
- 2020 - [kalendarzswiat.pl](https://www.kalendarzswiat.pl/niedziele_handlowe/2020)
"""

trade_sundays_df = pd.read_csv('/content/gdrive/MyDrive/Praca podyplomowa - Michał Lewandowski - Big Data CDV 2020 2021/data/trade sundays/trade sundays.csv')

trade_sundays_df['date'] = pd.to_datetime(trade_sundays_df['date'])

df = df.merge(trade_sundays_df,
              how = 'left',
              on = 'date')

df.head()

"""# Data cleaning

## Clean up location names
"""

df['location'].unique()

df['location'] = df['location'].str.lower()
df['location'] = df['location'].str.normalize('NFD')

chars_to_drop = ['.', ',', 'ul']
slashes_combo = [' / ', ' /', '/ ']

for char in chars_to_drop:  
  df['location'] = df['location'].str.replace(char, '')
for char in slashes_combo:  
  df['location'] = df['location'].str.replace(char, '/')
df['location'] = df['location'].str.replace('os ', 'osiedle ')
df['location'] = df['location'].str.replace('pl ', 'plac ')
df['location'] = df['location'].str.replace('al ', 'aleja ')
df['location'] = df['location'].str.replace('  ', ' ')
df['location'] = df['location'].str.replace(' ', '_')

df['location'].unique()

"""## Data type setting
Implicitly state data type for all columns.
"""

categorical_cols = ['location', 'drive_type', 'bike_type', 'child_seat', 'bike_size']
float_cols = ['Pśr', 'Pmax', 'Pmin', 'Tśr',
       'Tmaxśr', 'Tminśr', 'ABS Tmax', 'ABS Tmin', 'Uśr', 'Umin', 'ffśr', 'ffmax',
       'Nśr', 'Rdsuma', 'Rdmax', 'RD ≥ 0.1', 'RD ≥ 1.0', 'RD ≥ 5.0',
       'RD ≥ 10.0', 'RD ≥ 20.0', 'Tgminśr', 'ABS Tgmin', 'Deszcz',
       'Śnieg', 'Grad', 'Mgła', 'Zamglenie', 'Sadź', 'Gołoledź',
       'Zamieć śn. niska', 'Zamieć śn. wysoka', 'Zmętnienie opal.', 'Burza',
       'Rosa', 'Szron', 'Pokrywa śn.', 'Pręd. wiatru ≥10', 'Pręd. wiatru >15']
int_cols = ['action_count', 'bank_holidays_cnt', 'observances_cnt', 'equinoxes_cnt', 
            'Tmin ≤  0', 'Tmin ≤ -10',
            'Tmax ≤ -10', 'Tmax ≥ 25', 'Tmax ≥ 30',
            'trade_sundays_cnt']

df[categorical_cols] = df[categorical_cols].astype('category')
df[int_cols] = df[int_cols].fillna(value = 0)

for col in float_cols:
  df[col] = df[col].apply(lambda x: float(str(x).replace(',', '.')))
  df[col] = df[col].astype('float')

for col in int_cols:
  df[col] = df[col].astype('int')

"""Transform the date column into year and month columns, and then drop the date column afterwards"""

df['year'] = df['date'].apply(lambda x: x.year).astype('category')
df['month'] = df['date'].apply(lambda x: x.month).astype('category')

"""# EDA

### Data description & value counts
"""

df.describe(include = 'all')

df.info()

df.value_counts()

"""### Plotting

"""

sns.catplot(data = df, 
              x = 'action_count',
              y = 'action',
              hue = 'bike_type',
              kind = 'bar',
              col = 'drive_type')

plt.show()

"""#### Actions over time - line plot
Note: In the beginning of April 2020 the service was [temporarily shut down](https://poznanskirower.pl/zawieszenie-mozliwosc-wypozyczen-rowerow/) over safety concerns, only to [resume on May 6 2020](https://poznanskirower.pl/poznanski-rower-miejski-zostanie-ponownie-uruchomiony-o-polnocy-z-wtorku-na-srode-z-5-na-6-maja/).
"""

fig = sns.lineplot(data = df,
             x = 'month',
             y = df['action_count'],
             hue = 'year',
             style = 'action',
             markers = True)

fig.text(x = 4+0.1,
         y = 0-20,
         s = 'Service shutdown from Apr 1 to May 6', 
         color = 'orange')
         

plt.show()

"""#### Top and bottom stations - rents vs returns

#### Scatter plots - target vs feature variables
"""

data_cols = float_cols+int_cols

for col in data_cols: 
  plot = sns.scatterplot(x = df['action_count'],
                        y = df[col],
                        hue = df['action'],
                        style = df['bike_type'])
  plt.show()
  print('\n')

"""# Correlation
Build a correlation matrix for all possible features against  *action_count*
"""

# Build the correlation matrix
corr_matrix = df.corr()

# Set figure size
plt.rcParams['figure.figsize'] = (30,30)
sns.heatmap(abs(corr_matrix),
            annot=True)

"""# Feature engineering

## Multicollinearity check

### Filter out the unrevelant/weakly correlated features
Overall the features seem to be very poorly correlated, so set the bar low at:

```
0.15 > corr < 1
```
"""

# set the dependant variable correlation results to a var
corr_target = abs(corr_matrix['action_count'])

# select highly correlated features
relevant_features = corr_target[(corr_target > 0.15) & (corr_target < 1)].sort_values()
print(relevant_features)

"""### Check the correlation between the relevant features themselves"""

relevant_features_corr = df[relevant_features.index].corr()

sns.heatmap(abs(relevant_features_corr),
            annot=True)

"""### Get a list of features that have a corr rating of less than X - probably unrelated

"""

relevant_uncorrelated_features = relevant_features_corr[relevant_features_corr < 0.1].unstack().sort_values(ascending = False).dropna().drop_duplicates().reset_index(level=[0,1]).level_0.drop_duplicates().values

relevant_uncorrelated_features

"""# Build the final DFs"""

final_cols = ['year', 'month', 'action'] + categorical_cols + relevant_uncorrelated_features.tolist() + ['action_count']
df_final = df[final_cols]

rents_2019 = df_final[(df_final['year'] == 2019) & (df_final['action'] == 'rent')].drop(['year', 'action'], axis = 1)
returns_2019 = df_final[(df_final['year'] == 2019) & (df_final['action'] == 'return')].drop(['year', 'action'], axis = 1)

rents_2020 = df_final[(df_final['year'] == 2020) & (df_final['action'] == 'rent')].drop(['year', 'action'], axis = 1)
returns_2020 = df_final[(df_final['year'] == 2020) & (df_final['action'] == 'return')].drop(['year', 'action'], axis = 1)

dfs = [rents_2019,
       returns_2019,
       rents_2020,
       returns_2020]

"""# Modelling

## Modelling function
"""

def train_model(data, test_size = 0.25):
  X = data.drop('action_count',
                axis = 1)
  y = data['action_count']

  #encode categorical features
  X_enc = pd.get_dummies(X, columns = categorical_cols)
  X_enc.head()

  #train/test split
  X_train, X_test, y_train, y_test = train_test_split(X_enc, y,
                                                    test_size = test_size,
                                                    random_state = 1)
  
  X_train_scaled = scale(X_train)
  X_test_scaled = scale(X_test)

  #build and train the model
  model = SVR()
  model.fit(X_train,
            y_train)
  
  y_pred = model.predict(X_test)

  return model, y_test, y_pred

"""## Train models"""

rents_2019_model = train_model(rents_2019)
returns_2019_model = train_model(returns_2019)
rents_2020_model = train_model(rents_2020)
returns_2020_model = train_model(returns_2020)

"""## Score checking function"""

def print_model_scores(model):
  print('R2 score: '+str(r2_score(model[1],
         model[2])))
  print('Mean squared error: '+str(mean_squared_error(model[1],
         model[2])))
  print('Explained variance score: '+str(explained_variance_score(model[1],
         model[2])))
  print('-'*10)

[print_model_scores(x) for x in [rents_2019_model, returns_2019_model, rents_2020_model, returns_2020_model]]

y_test

"""# Do zbadania
- wiersze z zerowymi wartościami - dlaczego są?
- multicolinearity
- AIC
- scatter plot

# Modele
- SVM (supervised learning)
- klastrowanie (unsupervised learning)
- 

# Dane
- rozważyć wyrzucenie rowerków dziecięcych
- sprawdzić istotność stastystyczną, np. confirmatory factor analysis

# Struktura pracy
- wstęp (proces badawczy, metodologia, narzędzie pracy, raport tego co zrobiłem
- od 30 do 40 stron (+/-)
- raczej w wordzie, repo na gicie do kodu i danych

# Do zrobienia
- testy statystyczne
- wysłać Doktorowi zbiór danych w csv

jacek.nozewski@gmail.com

"""